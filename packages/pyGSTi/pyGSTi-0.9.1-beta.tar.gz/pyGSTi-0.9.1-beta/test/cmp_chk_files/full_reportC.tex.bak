\documentclass{article}[11pt]
\usepackage{longtable}
\usepackage{graphicx}
\usepackage{fix-cm}
\usepackage[margin=1in,paperwidth=8.5in,paperheight=11in]{geometry}
\usepackage[section]{placeins}
\usepackage{flafter}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{etoolbox}
\usepackage{units}
\usepackage{multirow}
\usepackage{adjustbox}

\setcounter{topnumber}{3}
\setcounter{bottomnumber}{3}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.9}
\renewcommand{\bottomfraction}{0.9}
\renewcommand{\textfraction}{0.1}
\renewcommand{\floatpagefraction}{0.7}

\newcommand{\rrangle}{\rangle\!\rangle} \newcommand{\llangle}{\langle\!\langle}
\newcommand{\ket}[1]{\ensuremath{\left|#1\right\rangle}}
\newcommand{\bra}[1]{\ensuremath{\left\langle#1\right|}}
\newcommand{\braket}[2]{\ensuremath{\left\langle#1|#2\right\rangle}}
\newcommand{\expec}[1]{\ensuremath{\left\langle#1\right\rangle}}
\newcommand{\ketbra}[2]{\ket{#1}\!\!\bra{#2}}
\newcommand{\braopket}[3]{\ensuremath{\bra{#1}#2\ket{#3}}}
\newcommand{\proj}[1]{\ketbra{#1}{#1}}
\newcommand{\sket}[1]{\ensuremath{\left|#1\right\rrangle}}
\newcommand{\sbra}[1]{\ensuremath{\left\llangle#1\right|}}
\newcommand{\sbraket}[2]{\ensuremath{\left\llangle#1|#2\right\rrangle}}
\newcommand{\sketbra}[2]{\sket{#1}\!\!\sbra{#2}}
\newcommand{\sbraopket}[3]{\ensuremath{\sbra{#1}#2\sket{#3}}}
\newcommand{\sproj}[1]{\sketbra{#1}{#1}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\def\Id{1\!\mathrm{l}}
\newcommand{\Tr}[0]{\mathrm{Tr}}
\providecommand{\e}[1]{\ensuremath{\times 10^{#1}}}

%Command used for python automatic substitution
\newcommand{\putfield}[2]{#2}

\newtoggle{confidences}
\newtoggle{LsAndGermsSet}
\newtoggle{debuggingaidsappendix}
\newtoggle{gaugeoptappendix}
\newtoggle{pixelplotsappendix}
\newtoggle{whackamoleappendix}
\togglefalse{confidences}
\toggletrue{LsAndGermsSet}
\togglefalse{debuggingaidsappendix}
\togglefalse{gaugeoptappendix}
\togglefalse{pixelplotsappendix}
\togglefalse{whackamoleappendix}



\begin{document}

\title{GST report for $\mathcal{D}$}
\date{\vspace{-1cm}\today}
%\author{}

\begingroup
\let\center\flushleft
\let\endcenter\endflushleft
\maketitle
\endgroup

\section{Overview}
This report presents a gate-set tomography (GST) analysis of a dataset called ``$\mathcal{D}$''.  

GST characterizes logic operations on a quantum device (e.g., a qubit), by treating it as a black box.  This black box is equipped with a small set of ``buttons'' that apply quantum \emph{gates} to the quantum system inside.  One button initializes it, a second button triggers a 2-outcome measurement, and the remaining buttons perform transformations.  We avoid assumptions about the device's operation whenever possible.  Currently, we assume that:
\begin{itemize}
\item the quantum device is a qubit (has a Hilbert space of dimension 2),
\item each \emph{gate}, or logic operation, can be represented by a stationary Markov process (a.ka. ``quantum channel'').
\end{itemize}
The core of GST is an algorithm that takes certain inputs, and produces certain outputs.  The \emph{input} to GST comprises (1) a list of data, and (2) ``target'' gateset describing the \emph{ideal} behavior of the device.  GST data comprises a list of experiments -- each described by the sequence of gates that was applied -- and, for each experiment, two integer \emph{counts} stating how often the ``plus'' and ``minus'' results were observed.  The target gates are used \emph{only} to (a) report how consistent the estimates are with the target, and (b) choose the best \emph{gauge} in which to report the results.  GST does not take them into account in its core analysis, and there is no possibility of circularity or other ``cheating''.

GST's primary output is an estimated \emph{gateset} that models or fits the device's observed behavior.  Gatesets are of the form $\{\rho_0,E_0,\{G_k\}\}$, where
\begin{itemize}
\item $\rho_0$ is an estimate of the density matrix in which the device gets initialized,
\item $\{E_0,\Id-E_0\}$ is an estimate of the POVM describing how it gets measured,
\item and each of the $G_k$ is an estimate of the superoperator (quantum process) describing the corresponding gate.
\end{itemize}
Unless something went wrong (usually it doesn't), the output of GST is the best possible fit to the data.  This should also mean that they are a very accurate description of what happens when you trigger a gate on your device.  However, this happy conclusion relies on two assumptions:
\begin{enumerate}
\item The experiments were chosen wisely, so that the only gate sets consistent with their results are very close to the true behavior.  This is usually true.  The main failure mode occurs when you were not able to perform \emph{long} sequences (e.g., because your decoherence rate is very high), in which case accuracy may be limited. 
\item The operations you are performing really are stationary (time-independent), Markovian, and acting on a quantum system with the correct Hilbert space dimension.  These assumptions define the \emph{model} that GST fits to the data.  \textbf{They are usually not true!}  Quantum operations are usually at least a little bit non-Markovian.  In this report (Section \ref{secGoodness}) we provide extensive self-checks to identify and diagnose violations of the model.  If your system \emph{is} visibly non-Markovian, then (a) these checks will probably warn you of it, and (b) the other quantities reported here should be treated with caution -- using GST on non-Markovian gates violates the warranty!
\end{enumerate}

This document is organized into three main sections, which address three broad questions.
\begin{itemize}
\item Section \ref{secInput}:  What inputs did you give GST?
\item Section \ref{secOutput}:  What estimate did GST output, and what does it mean?
\item Section \ref{secGoodness}:  How reliable are the results? (How badly was the model violated?)
\end{itemize}
Section \ref{secInput} is primarily useful to verify that the inputs were correct.  Section \ref{secOutput} is the most important:  it presents the raw estimates derived by the GST algorithm, and also provides a variety of derived quantities that may be useful in interpreting what this estimate means.

Section \ref{secGoodness}) is dedicated to summarizing how well the model imposed by GST was able to fit the data, relative to what is expected of a ``good'' model.  This is \emph{not} related to ``How close is the GST estimate to the target gates?'', which is addressed in Section \ref{secOutput}.  It is also not the same as ``How large are the error bars on the GST estimate?'', which is a good question that we do not directly address at this time.  Instead, Section \ref{secGoodness} is intended to tell you whether (a) you should take the GST estimate at face value, or (b) it should be treated skeptically because \emph{no} gate set was capable of fitting the data.

Finally, appendices may be present (depending on which options were chosen when this report was generated).  Appendices present more detailed debugging information, elaborating on the goodness-of-fit metrics presented in Section \ref{secGoodness}.

\section{Input Summary\label{secInput}}
The input for this GST analysis comprised: (1) a target gateset (see Tables \ref{targetSpamTable}-\ref{targetGatesTable}); and (2) a dataset called ``$\mathcal{D}$''.

\subsection{Target Gateset}

The target gateset describes the ideal initial state (density matrix), measurement (POVM effect), and gate operations (superoperators).  Typically, density matrices and POVM effects are represented as square $d\times d$ matrices on a Hilbert space $\mathcal{H}$.  In GST, it is often more convenient to represent them as $d^2$-element vectors in the Hilbert-Schmidt space $\mathcal{B}(\mathcal{H})$ of linear operators on $\mathcal{H}$.  Both representations are shown in Table \ref{targetGatesTable}.  Superoperators are sometimes represented in Choi or Kraus form, but for GST it is more convenient to represent them as square $d^2\times d^2$ matrices that multiply associatively and act on $\mathcal{B}(\mathcal{H})$.  These are shown in Table \ref{targetGatesTable}.

These Hilbert-Schmidt space representations require choosing a basis $\{M_i\}$ for $\mathcal{B}(\mathcal{H})$.  We use the \emph{Pauli basis}, comprising the four $2\times2$ Pauli matrices (including the identity $\Id$) for $d=2$.  In $d>2$, we use the analogous Gell-Mann matrices as a basis.   The choice of this basis is what is meant when state preparations and measurements are written as vectors and gate operations are written as matrices in the ``Pauli basis''.  Keep in mind that we want to use an orthonormal basis, so the basis matrices are normalized so that $\sbraket{M_i}{M_j} = \Tr M_i^\dagger M_j = \delta_{ij}$.  In $d=2$, this means that the basis matrices are $M_i = \frac{1}{\sqrt{2}}\sigma_i$.

\begin{table}[h]
\begin{center}
\begin{tabular}[l]{|c|c|c|}
\hline
Operator & Hilbert-Schmidt vector (Pauli basis) & Matrix \\ \hline
$\rho_{0}$ & $ \begin{array}{c}
0.7071 \\ 
0 \\ 
0 \\ 
0.7071
 \end{array} $
 & $ \left(\!\!\begin{array}{cc}
1 & 0 \\ 
0 & 0
 \end{array}\!\!\right) $
 \\ \hline
$E_{0}$ & $ \begin{array}{c}
0.7071 \\ 
0 \\ 
0 \\ 
-0.7071
 \end{array} $
 & $ \left(\!\!\begin{array}{cc}
0 & 0 \\ 
0 & 1
 \end{array}\!\!\right) $
 \\ \hline
\end{tabular}

\caption{\textbf{Target gateset: SPAM (state preparation and measurement) gates}.  These are the \emph{ideal} input state ($\rho_0$) and `plus' POVM effect $E_0$ for the device on which we report.  SPAM gates are given here both as $d\times d$ matrices, and in ``vectorized'' form as $d^2$-dimensional vectors in $\mathcal{B}(\mathcal{H})$.  See Table \ref{bestGatesetSpamTable} for GST estimates of the actual $\rho_0$ and $E_0$ implemented in this experiment.\label{targetSpamTable}}
\end{center}
\end{table}

The ideal SPAM operations for your particular case are given in Table \ref{targetSpamTable}.  The ideal \emph{logic gate} operations are given, as superoperators written in the Pauli basis, in Table \ref{targetGatesTable}.

In most cases, the ideal/target logic gates are reversible unitary rotations.  The corresponding superoperators are orthogonal rotations on $\mathcal{B}(\mathcal{H})$.  For your convenience, Table \ref{targetGatesTable} also lists (for each logic gate) an axis of rotation [as a vector in $\mathcal{B}(\mathcal{H})$] and an angle of rotation.  

\begin{table}[h]
\begin{center}
\begin{tabular}[l]{|c|c|c|c|}
\hline
Gate & Superoperator (Pauli basis) & Rotation axis & Angle \\ \hline
Gi & $ \left(\!\!\begin{array}{cccc}
1 & 0 & 0 & 0 \\ 
0 & 1 & 0 & 0 \\ 
0 & 0 & 1 & 0 \\ 
0 & 0 & 0 & 1
 \end{array}\!\!\right) $
 & $ \begin{array}{c}
0 \\ 
1 \\ 
0 \\ 
0
 \end{array} $
 & 0$\pi$ \\ \hline
Gx & $ \left(\!\!\begin{array}{cccc}
1 & 0 & 0 & 0 \\ 
0 & 1 & 0 & 0 \\ 
0 & 0 & 0 & -1 \\ 
0 & 0 & 1 & 0
 \end{array}\!\!\right) $
 & $ \begin{array}{c}
0 \\ 
1 \\ 
0 \\ 
0
 \end{array} $
 & 0.5$\pi$ \\ \hline
Gy & $ \left(\!\!\begin{array}{cccc}
1 & 0 & 0 & 0 \\ 
0 & 0 & 0 & 1 \\ 
0 & 0 & 1 & 0 \\ 
0 & -1 & 0 & 0
 \end{array}\!\!\right) $
 & $ \begin{array}{c}
0 \\ 
0 \\ 
1 \\ 
0
 \end{array} $
 & 0.5$\pi$ \\ \hline
\end{tabular}

\caption{\textbf{Target gateset: logic gates}.  These are the \emph{ideal} (generally unitary) logic gates.  Each has a name starting with ``G'', and is represented as a $d^2\times d^2$ \emph{superoperator} that acts by matrix multiplication on vectors in $\mathcal{B}(\mathcal{H})$.  For each gate, its axis of rotation (in $\mathcal{B}(\mathcal{H})$ and angle of rotation are also given.  See Table \ref{bestGatesetGatesTable} for GST estimates of the actual logic gates implemented in this experiment.\label{targetGatesTable}}
\end{center}
\end{table}

\subsection{GST Input Data}

The most important input to GST is a \emph{dataset} -- a list of experimental counts or frequencies, each associated with a \emph{gate sequences}.  Gate sequences are also referred to as ``gate strings''.  Each gate sequence defines an experiment, in which you (1) initialize the device, (2) apply the operations specified by the gate sequence, and (3) measure and record the result (``plus'' or ``minus'').

Typically, the gate sequences that appear in the dataset are generated by the following process:
\begin{enumerate}
\item A small set of short gate sequences called \emph{germs} are chosen,
\item A small set of short \emph{fiducial sequences} are chosen so that, when applied to $\rho_0$ or $E_0$, they generate an informationally complete set of states or effects.
\item Each germ is concatenated with itself to form \emph{base sequences} of length approximately $1,2,4,8,\ldots L_{max}$.
\item Each base sequence is sandwiched between every possible pair of fiducial sequences.
\end{enumerate}
The dataset comprises all sandwiched base sequences.  A few other short sequences (e.g., those corresponding to the empty base sequence) may also appear.

\iftoggle{LsAndGermsSet}{ The fiducial sequences and germs for \emph{this} dataset are given in Table \ref{fiducialAndGermListTables}. }{ Fiducial sequence and germ information was not given for this report, and may not be applicable.}  An overview of the information contained in the file you provided for dataset ``$\mathcal{D}$'' is given in Table \ref{datasetOverviewTable}.  

This table also contains one derived quantity, the spectrum of the largest \emph{Gram matrix} that GST could extract from the data.  This is included here rather than in the analysis because it is not useful for predictive purposes, and therefore is not part of the estimate.  It serves, instead, to tell you something about the quality of the data.  More precisely, it tells you about the dimension of the state space that is explored by the fiducial sequences.  This should be $d^2$-dimensional [because the fiducials are intended to explore all of $\mathcal{B}(\mathcal{H})$], and therefore the spectrum listed in Table \ref{datasetOverviewTable} should (ideally) have exactly $d^2$ elements that are large and nonzero.  In practice, you should see $d^2$ large elements, and a rapid drop in magnitude thereafter.  If fewer than $d^2$ elements are large, then the fiducials were poorly chosen and are not exploring the state space effectively.  If more than $d^2$ are large, then the system is experiencing strong non-Markovian effects (e.g., strong coupling to environmental degrees of freedom) or it has a larger Hilbert space dimension than expected.

\iftoggle{LsAndGermsSet}{

\begin{table}[h]
\begin{center}
\begin{minipage}[b]{0.40\linewidth}
\centering
\adjustbox{max width=\linewidth}{
\begin{tabular}[l]{|c|c|c|}
\hline
 & \multicolumn{2}{c|}{Fiducials} \\ \hline
\# & Prep. & Measure \\ \hline
1 & $$ & $$ \\ \hline
2 & $\mbox{Gx}$ & $\mbox{Gx}$ \\ \hline
3 & $\mbox{Gy}$ & $\mbox{Gy}$ \\ \hline
4 & $\mbox{Gx}\cdot\mbox{Gx}$ & $\mbox{Gx}\cdot\mbox{Gx}$ \\ \hline
5 & $\mbox{Gx}\cdot\mbox{Gx}\cdot\mbox{Gx}$ & $\mbox{Gx}\cdot\mbox{Gx}\cdot\mbox{Gx}$ \\ \hline
6 & $\mbox{Gy}\cdot\mbox{Gy}\cdot\mbox{Gy}$ & $\mbox{Gy}\cdot\mbox{Gy}\cdot\mbox{Gy}$ \\ \hline
\end{tabular}

}
%putfield{rhoStrListTable}{List of state prep fiducials table will be placed here}
%putfield{EStrListTable}{List of effect fiducials table will be placed here}
\end{minipage}
\begin{minipage}[b]{0.40\linewidth}
\adjustbox{max width=\linewidth}{
\begin{tabular}[l]{|c|c|}
\hline
\# & Germ \\ \hline
1 & $\mbox{Gx}$ \\ \hline
2 & $\mbox{Gy}$ \\ \hline
3 & $\mbox{Gi}$ \\ \hline
4 & $\mbox{Gx}\cdot\mbox{Gy}$ \\ \hline
5 & $\mbox{Gx}\cdot\mbox{Gy}\cdot\mbox{Gi}$ \\ \hline
6 & $\mbox{Gx}\cdot\mbox{Gi}\cdot\mbox{Gy}$ \\ \hline
7 & $\mbox{Gx}\cdot\mbox{Gi}\cdot\mbox{Gi}$ \\ \hline
8 & $\mbox{Gy}\cdot\mbox{Gi}\cdot\mbox{Gi}$ \\ \hline
9 & $\mbox{Gx}\cdot\mbox{Gx}\cdot\mbox{Gi}\cdot\mbox{Gy}$ \\ \hline
10 & $\mbox{Gx}\cdot\mbox{Gy}\cdot\mbox{Gy}\cdot\mbox{Gi}$ \\ \hline
11 & $\mbox{Gx}\cdot\mbox{Gx}\cdot\mbox{Gy}\cdot\mbox{Gx}\cdot\mbox{Gy}\cdot\mbox{Gy}$ \\ \hline
\end{tabular}

}
\end{minipage}
\caption{\textbf{Fiducial sequences and germs.}  See discussion in text.\label{fiducialAndGermListTables}}
\end{center}
\end{table}

}{}

\begin{table}[h]
\begin{center}
\begin{tabular}[l]{|c|c|}
\hline
Quantity & Value \\ \hline
Number of strings & 817 \\ \hline
Gate labels & Gx, Gy, Gi \\ \hline
SPAM labels & plus, minus \\ \hline
Gram singular vals & \small$ \begin{array}{c}
0.0029 \\ 
0.0205 \\ 
0.6626 \\ 
0.6789 \\ 
0.7472 \\ 
3.0074
 \end{array} $
 \\ \hline
\end{tabular}

\caption{\textbf{General dataset properties}.  See discussion in text.\label{datasetOverviewTable}}
\end{center}
\end{table}

\section{Output from GST\label{secOutput}}

The primary output of GST is an estimated gateset.  This section presents the raw estimate, and then some useful derived quantities of the estimated gates, including comparisons to the target gates.

\subsection{Raw GST estimates}

Table \ref{bestGatesetSpamTable} reports the estimated SPAM operations, and Table \ref{bestGatesetGatesTable} reports the logic gate operations.  The estimated SPAM gates ($\rho_0$ and $E_0$) are vectors in $\mathcal{B}(\mathcal{H})$, and the estimated logic gates are superoperators represented as matrices acting on $\mathcal{B}(\mathcal{H})$, all in the Pauli basis.   By taking the dot product of state preparation and measurement vectors estimated SPAM probabilites are computed in Table \ref{bestGatesetSpamParametersTable}.
\iftoggle{confidences}{
Tables \ref{bestGatesetSpamTable} and \ref{bestGatesetGatesTable} report NOT-SET\% confidence intervals for each of the gate matrix and SPAM vector elements.  A confidence region is obtained by approximating the log-likelihood (see below) as being quadratic about its minimum, and determining the ellipsoid where this approximation equals a value, $C$, defined below.  For a given parameter (e.g. gate or SPAM vector element) $x$, a confidence interval is obtained by projecting the ellipsiodal region onto that $x$'s axis.  This computes a 1-dimensional NOT-SET\% confidence interval for the profile log-liklihood for $x$, and for this reason the value of $C$ used above is chosen such that $\mathrm{CDF}[\chi^2_1](C) = NOT-SET\%$ (that is, at the value $C$ the cumulative density function of a $\chi^2_1$ distribution reaches NOT-SET\%).  If, instead, the interval corresponding to a projection of the NOT-SET\% multi-dimensional confidence region (defined by $C$ s.t.~$\mathrm{CDF}[\chi^2_n](C) = NOT-SET\%$, where $n=NOT-SET$ is the number of non-gauge gate set parameters) is desired, then the all the interval widths reported here should be multiplied by NOT-SET.  The resulting confidence interval is always symmetric about the estimated value, and we report the half-width of the intervals in the tables.  In table \ref{bestGatesetSpamParametersTable} and those in the following section, we specify the NOT-SET\% confidence intervals of derived quantities in using \emph{value} $\pm$ \emph{half-width} notation.  The derived-quantity confidence intervals in section \ref{derivedQtySection} are computed by finding the minimum and maximum values of the linearization of the derived quantity (e.g.~fidelity).
}{}


\begin{table}[h]
\begin{center}
\begin{tabular}[l]{|c|c|c|}
\hline
Operator & Hilbert-Schmidt vector (Pauli basis) & Matrix \\ \hline
$\rho_{0}$ & $ \begin{array}{c}
0.7071 \\ 
0.0016 \\ 
0.0003 \\ 
0.6396
 \end{array} $
 & $ \left(\!\!\begin{array}{cc}
0.9523 & 0.0012e^{-i0.2} \\ 
0.0012e^{i0.2} & 0.0477
 \end{array}\!\!\right) $
 \\ \hline
$E_{0}$ & $ \begin{array}{c}
0.7078 \\ 
-0.0002 \\ 
-0.0031 \\ 
-0.6407
 \end{array} $
 & $ \left(\!\!\begin{array}{cc}
0.0475 & 0.0022e^{i1.6} \\ 
0.0022e^{-i1.6} & 0.9535
 \end{array}\!\!\right) $
 \\ \hline
\end{tabular}

\caption{\textbf{The GST estimate of the SPAM operations}.  Compare to Table \ref{targetSpamTable}.\label{bestGatesetSpamTable}}
\end{center}
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}[l]{|c|c|c|}
\hline
 & $E_{0}$ & $E_C$ \\ \hline
$\rho_{0}$ & 0.090701 & 0.909299 \\ \hline
\end{tabular}

\caption{\textbf{GST estimate of SPAM probabilities}.  Computed by taking the dot products of vectors in Table \ref{bestGatesetSpamTable}.\label{bestGatesetSpamParametersTable}}
\end{center}
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}[l]{|c|c|}
\hline
Gate & Superoperator (Pauli basis) \\ \hline
Gi & $ \left(\!\!\begin{array}{cccc}
1 & 0 & 0 & 0 \\ 
-0.001 & 0.9447 & -0.0063 & -0.0088 \\ 
0.0004 & 0.0011 & 0.9471 & 0.0038 \\ 
0.0011 & 3\e{-5} & 0.0007 & 0.9485
 \end{array}\!\!\right) $
 \\ \hline
Gx & $ \left(\!\!\begin{array}{cccc}
1 & 0 & 0 & 0 \\ 
0.0003 & 0.9505 & -0.0049 & -0.0054 \\ 
0.0003 & 0.0002 & 0.0001 & -0.9491 \\ 
0.0002 & 0.0003 & 0.949 & -0.0022
 \end{array}\!\!\right) $
 \\ \hline
Gy & $ \left(\!\!\begin{array}{cccc}
1 & 0 & 0 & 0 \\ 
0.0002 & 0.0001 & -0.004 & 0.9488 \\ 
0.0003 & 0.0011 & 0.9512 & 0.0011 \\ 
-0.0001 & -0.949 & -0.0062 & -0.0005
 \end{array}\!\!\right) $
 \\ \hline
\end{tabular}

\caption{\textbf{The GST estimate of the logic gate operations}.  Compare to Table \ref{targetGatesTable}.\label{bestGatesetGatesTable}}
\end{center}
\end{table}

The estimated gates can be compared directly to the target gateset given in Section \ref{secInput}.  Ideally, they would match.  In practice, of course, they won't.  One of the best ways we have found to evaluate the significance of discrepancies is to compare \emph{derived} quantities -- i.e., certain properties calculated from the gate matrices and SPAM vectors.  Deriving quantities from these raw outputs occupies the remainder of this section.

\subsection{Derived quantities\label{derivedQtySection}}

Generally, the first thing that you want to know is ``How far from ideal are the gates?''  To answer this, this report tabulates several well-known metrics of distance.  Table \ref{bestGatesetVsTargetTable} lists the discrepancy from each estimated gate to its corresponding target, as measured by:
\begin{enumerate}
\item \textbf{Process infidelity}.  Infidelity is simply $1-F$, where $F$ is a \emph{fidelity}.  The process fidelity between quantum processes $G_a$ and $G_b$ is given by $F = \Tr\left( \sqrt{ \sqrt(\chi_a) \chi_b \sqrt(\chi_a) } \right)^2$, where $\chi_a$ and $\chi_b$ are the Jamiolkowski states (normalized Choi process matrices) corresponding to gate matrices $G_a$ and $G_b$ respectively.  If the target is unitary (as is often the case), $F = \Tr\left( \chi_a \chi_b \right)$.  Process infidelity is roughly what is measured in randomized benchmarking protocols; it quantifies the \emph{incoherent} error rate if coherent errors (e.g. over rotations) are not allowed to accumulate.
\item \textbf{Trace distance}.  This is the \emph{Jamiolkowski trace distance} between the Jamiolkowski states corresponding to the two processes:  $d_{tr} = \vert\chi_a - \chi_b\vert_1 = \Tr\left(\sqrt{(\chi_a-\chi_b)^2}\right)$.  This distance is useful primarily as a proxy for the \emph{diamond norm distance}, because $d_{tr} \leq d_{\diamond} \leq \mathrm{dim}(\mathcal{H}) d_{tr}$.  The diamond norm distance is an upper bound on the rate of error under any possible circumstance (including coherent accumulation of errors) and is often used in proofs of fault tolerance.  For gates dominated by coherent/unitary error, it is common to see $d_{\diamond} \approx \sqrt{1-F}$.  For gates dominated by incoherent error, $d_{\diamond} \approx 1-F$.
\item \textbf{Diamond Norm}.  The diamond norm between two quantum processes $G_a$ and $G_b$ is given by $\norm{G_a - G_b}_\Diamond = \sup_\rho \norm{(G_a \otimes I_k)(\rho) - (G_b \otimes I_k)(\rho)}_1$, where $I_k$ is the $k$-dimensional identity operation, $\norm{\cdot}_1$ denotes the trace norm, and the supremum is taken over all $k \ge 1$ and density matrices $\rho$ of dimension $nk$, with $n$ the dimension of $G_a$ and $G_b$.  The diamond norm is also called the \emph{completely bounded trace norm}, and plays the analogous role for quantum process distinguishability that the trace norm plays for density matrices.  Specifically, the optimal probability of distinguishing $G_a$ from $G_b$ after a \emph{single evaluation} is given by $\frac{1}{2} + \frac{1}{4}\norm{G_a - G_b}_\Diamond$.
\item \textbf{Frobenius-norm distance}.  The Frobenius norm distance between two gates $G_a$ and $G_b$ is simply $d_F = \sqrt{\Tr\left[\left(G_a-G_b\right)^2\right]}$.  It has no known \emph{operational} interpretation, but is very convenient as a rough measure of inaccuracy.  It is also equal to the sum of the RMS errors in the individual matrix elements of the gates.
\end{enumerate}

It's also useful to know \emph{how} the real gates (or, more precisely, GST's estimates of the real gates) differ from the targets.  There are several ways we could represent this, but the most useful involves an \emph{error generator}.  These are also given in Table \ref{bestGatesetVsTargetTable}.  The final column of the table lists, for each gate, a Lindbladian superoperator $\mathbb{L}$.  It is defined by the equation $\hat{G} = G_{\mathrm{target}}e^{\mathbb{L}}$, where $\hat{G}$ is the estimate and $G_{\mathrm{target}}$ is the ideal gate.  This Lindbladian would be zero if the gates were perfect, and its overall magnitude is approximately equal to the diamond distance (or Jamiolkowski trace distance) between the target gate and the estimate.

\begin{table}[h]
\begin{center}
\begin{tabular}[l]{|c|c|c|c|c|}
\hline
Gate & \begin{tabular}{c}Process\\Infidelity\end{tabular} & \begin{tabular}{c}$\nicefrac{1}{2}$ Trace\\Distance\end{tabular} & $\nicefrac{1}{2}$ $\Diamond$-Norm & \begin{tabular}{c}Frobenius\\Distance\end{tabular} \\ \hline
Gi & 0.039929 & 0.0401 & 0.040106 & 0.092986 \\ \hline
Gx & 0.037847 & 0.037919 & 0.03792 & 0.087746 \\ \hline
Gy & 0.037738 & 0.037804 & 0.037804 & 0.0875 \\ \hline
\end{tabular}

\vspace{2em}
\begin{tabular}[l]{|c|c|}
\hline
Gate & Error Generator \\ \hline
Gi & $ \left(\!\!\begin{array}{cccc}
0 & 0 & 0 & 0 \\ 
-0.001 & -0.0569 & -0.0066 & -0.0093 \\ 
0.0004 & 0.0011 & -0.0543 & 0.0041 \\ 
0.0011 & 3\e{-5} & 0.0008 & -0.0529
 \end{array}\!\!\right) $
 \\ \hline
Gx & $ \left(\!\!\begin{array}{cccc}
0 & 0 & 0 & 0 \\ 
0.0003 & -0.0508 & -0.0051 & -0.0057 \\ 
0.0002 & 0.0003 & -0.0523 & -0.0023 \\ 
-0.0003 & -0.0003 & -0.0001 & -0.0523
 \end{array}\!\!\right) $
 \\ \hline
Gy & $ \left(\!\!\begin{array}{cccc}
0 & 0 & 0 & 0 \\ 
0.0001 & -0.0524 & 0.0065 & 0.0005 \\ 
0.0003 & 0.0011 & -0.05 & 0.0012 \\ 
0.0002 & 0.0001 & -0.0042 & -0.0525
 \end{array}\!\!\right) $
 \\ \hline
\end{tabular}

\caption{\textbf{Comparison of GST estimated gates to target gates}.  This table presents, for each of the gates, three different measures of distance or discrepancy from the GST estimate to the ideal target operation.  See text for more detail.  The second table lists the ``Error Generator'' for each gate, which is the Lindbladian $\mathbb{L}$ that describes \emph{how} the gate is failing to match the target.  This error generator is defined by the equation $\hat{G} = G_{\mathrm{target}}e^{\mathbb{L}}$. \label{bestGatesetVsTargetTable}}
\end{center}
\end{table}

It's usually useful to understand \emph{how} gates fail.  The error generators in Table \ref{bestGatesetVsTargetTable} provide one view on this, but they are not necessarily intuitive.   For example, you might want to know whether your gate suffers depolarizing, dephasing, or over-rotation errors.  In Table \ref{bestGatesetDecompTable}, the estimated gates are decomposed into: (1) rotations (including angle and axis errors); (2) incoherent \emph{diagonal} decay rates (depolarizing or $T_1$ noise); and (3) incoherent \emph{off-diagonal} decay rates (dephasing or $T_2$ noise).  These analyses can be compared with a the similar decomposition of the target gates (cf. table \ref{targetGatesTable}).  Note that for some erroneous gates, this decomposition simply fails; if the numbers make no sense, this is probably the case.

\begin{table}[h]
\small
\begin{center}
\begin{tabular}[l]{|c|c|c|c|c|c|}
\hline
Gate & Eigenvalues & Fixed pt & Rotn. axis & Diag. decay & Off-diag. decay \\ \hline
Gi & $ \begin{array}{c}
0.9487 \\ 
0.9458e^{i0.0} \\ 
0.9458e^{-i0.0} \\ 
1
 \end{array} $
 & $ \begin{array}{c}
0.999 \\ 
-0.0223 \\ 
0.0093 \\ 
0.0206
 \end{array} $
 & $ \begin{array}{c}
0 \\ 
0.9348 \\ 
-0.1473 \\ 
-0.3233
 \end{array} $
 & 0.05127 & 0.054222 \\ \hline
Gx & $ \begin{array}{c}
0.9491e^{i1.6} \\ 
0.9491e^{-i1.6} \\ 
0.9505 \\ 
1
 \end{array} $
 & $ \begin{array}{c}
1 \\ 
0.0064 \\ 
3\e{-5} \\ 
0.0002
 \end{array} $
 & $ \begin{array}{c}
0 \\ 
-1 \\ 
3\e{-5} \\ 
-0.0003
 \end{array} $
 & 0.049514 & 0.050938 \\ \hline
Gy & $ \begin{array}{c}
0.9489e^{i1.6} \\ 
0.9489e^{-i1.6} \\ 
0.9512 \\ 
1
 \end{array} $
 & $ \begin{array}{c}
1 \\ 
1\e{-5} \\ 
0.0067 \\ 
-0.0002
 \end{array} $
 & $ \begin{array}{c}
0 \\ 
-0.0054 \\ 
1 \\ 
-0.0012
 \end{array} $
 & 0.048788 & 0.051084 \\ \hline
\end{tabular}

\vspace{2em}
\begin{tabular}[l]{|c|c|c|c|c|}
\hline
\multirow{2}{*}{Gate} & \multirow{2}{*}{Angle} & \multicolumn{3}{c|}{Angle between Rotation Axes} \\ \cline{3-5}
 & & Gi & Gx & Gy \\ \hline
Gi & 0.000625$\pi$ &  & 0.884316$\pi$ & 0.548542$\pi$ \\ \hline
Gx & 0.500356$\pi$ & 0.884316$\pi$ &  & 0.498284$\pi$ \\ \hline
Gy & 0.50007$\pi$ & 0.548542$\pi$ & 0.498284$\pi$ &  \\ \hline
\end{tabular}

\caption{\textbf{Eigen-decomposition of estimated gates}.  Each estimated gate is described in terms of: (1) the eigenvalues of the superoperator; (2) the gate's fixed point (as a vector in $\mathcal{B}(\mathcal{H})$, in the Pauli basis); (3)  the axis around which it rotates, as a vector in $\mathcal{B}(\mathcal{H})$; (4) the angle of the rotation that it applies; (5) the decay rate along the axis of rotation (``diagonal decay''); (6) the decay rate perpendicular to the axis of rotation (``off-diagonal decay''); and (7) the angle between each gate's rotation axis and the rotation axes of the other gates.  ``X'' indicates that the decomposition failed or couldn't be interpreted. \label{bestGatesetDecompTable}}
\end{center}
\end{table}

It might be useful to know the closest \emph{unitary} operation to the estimated gate, and how close it is.  Usually, you were trying to implement a unitary.  If the closest unitary to $G$ was indeed $G_{\mathrm{target}}$, then all errors are incoherent; if not, you might be able to tweak the gate parameters to get closer relatively easily.  Also, implementing a particular unitary may be less important than just achieving \emph{some} set of mutually independent unitaries.  In these and other cases, the distance from an estimated gate to its closest unitary approximation is of interest.

Table \ref{bestGatesetClosestUnitaryTable} lists, for each estimated gate, the properties of its closest unitary approximation.  The table defines the closest unitary, in terms of an axis and angle (in $\mathcal{B}(\mathcal{H})$) of rotation.  It also presents the process fidelity and Jamiolkowski trace distance between the estimated gate and its closest unitary approximation.  A sanity check is computed by comparing the fidelity of the obtained closest unitary with a theoretical upper bound (if a value greater than one appears in this column then the other values in that row may be inaccurate).  If these numbers are similar to those in Table \ref{bestGatesetVsTargetTable}, then the gates are as close to the targets as they are to \emph{any} unitary.

\begin{table}[h]
\begin{center}
\begin{tabular}[l]{|c|c|c|c|c|c|}
\hline
Gate & \begin{tabular}{c}Process\\Infidelity\end{tabular} & \begin{tabular}{c}$\nicefrac{1}{2}$ Trace\\Distance\end{tabular} & \begin{tabular}{c}Rotation\\Axis\end{tabular} & \begin{tabular}{c}Rotation\\Angle\end{tabular} & Sanity \checkmark \\ \hline
Gi & 0.03992 & 0.039923 & $ \begin{array}{c}
0 \\ 
-0.2621 \\ 
-0.7412 \\ 
0.618
 \end{array} $
 & 0.002002$\pi$ & 0.000004 \\ \hline
Gx & 0.037844 & 0.037844 & $ \begin{array}{c}
0 \\ 
1 \\ 
-0.0027 \\ 
-2\e{-6}
 \end{array} $
 & 0.500357$\pi$ & 4\e{-7} \\ \hline
Gy & 0.037735 & 0.037735 & $ \begin{array}{c}
0 \\ 
0.0027 \\ 
-1 \\ 
1\e{-5}
 \end{array} $
 & 0.500072$\pi$ & 3\e{-7} \\ \hline
\end{tabular}

\caption{Information pertaining to the closest unitary gate to each of the estimated gates.\label{bestGatesetClosestUnitaryTable}}
\end{center}
\end{table}


Finally, Table \ref{bestGatesetChoiTable} presents each estimated gate's \emph{Choi matrix}, along with its spectrum.  The Choi matrix (sometimes ambiguously referred to as the ``process matrix'') is an alternative way to describe a process.  We usually prefer the ``superoperator representation'', which has the very useful property that the process matrix corresponding to applying $G_a$ and then $G_b$ is simply $G_bG_a$.  This is completely false for the Choi representation.  Nonetheless, the Choi representation is often useful, so we present it here -- but without a detailed discussion of its properties (see, e.g. the textbook by Nielsen and Chuang).

The Choi matrix $\chi(G)$ for a gate $G$ can be simply understood in either of two ways.  First, it is equivalent (up to choice of basis) to the \emph{Jamiolkowski state} defined by applying $G$ to one half of a maximally entangled bipartite state.  Second, it is the general (non-diagonal) form of the well-known Kraus representation, $G[\rho] = \sum_i{K_i\rho K_i^\dagger}$.  The Choi matrix behaves in many ways like a quantum state, and appears naturally in expressions for the process fidelity and Jamiolkowsi trace distance just as density matrices would enter these expressions when computing differences between states.  

Additionally, the condition of \emph{complete positivity} or CP (which all real quantum processes must satisfy) is simply the positivity of the Choi matrix.  Thus, negative eigenvalues in Table \ref{bestGatesetChoiTable} indicate that the estimate violates complete positivity.  If they are very small, they may simply indicate statistical fluctuations (unitary gates have $\chi$ matrices with zero eigenvalues, so any small fluctuation is likely to violate CP).  If they are large, they serve as a warning that (1) the model of CPTP maps is probably violated (usually because of non-Markovian behavior), and (2) this estimate may produce negative or greater-than-unity probabilities.  GST does \emph{not} generally impose complete positivity (although it is an option), precisely because violation of CP is a warning flag for non-Markovian behavior (which is very common in experimental qubits).

\begin{table}[h]
\begin{center}
\begin{tabular}[l]{|c|c|c|}
\hline
Gate & Choi matrix (Pauli basis) & Eigenvalues \\ \hline
Gi & $ \left(\!\!\begin{array}{cccc}
0.9601 & 0.0008e^{i1.9} & 0.0022e^{-i1.6} & 0.0019e^{-i1.4} \\ 
0.0008e^{-i1.9} & 0.0123 & 0.0013e^{-i0.2} & 0.0022e^{-i3.1} \\ 
0.0022e^{i1.6} & 0.0013e^{i0.2} & 0.0135 & 0.0012e^{i2.9} \\ 
0.0019e^{i1.4} & 0.0022e^{i3.1} & 0.0012e^{-i2.9} & 0.0142
 \end{array}\!\!\right) $
 & $ \begin{array}{c}
0.0107 \\ 
0.0127 \\ 
0.0165 \\ 
0.9601
 \end{array} $
 \\ \hline
Gx & $ \left(\!\!\begin{array}{cccc}
0.4871 & 0.4745e^{-i1.6} & 0.0014e^{-i1.6} & 0.0013e^{-i1.5} \\ 
0.4745e^{i1.6} & 0.4882 & 0.0012e^{-i0.0} & 0.0013e^{-i3.1} \\ 
0.0014e^{i1.6} & 0.0012e^{i0.0} & 0.013 & 0.0001e^{-i1.5} \\ 
0.0013e^{i1.5} & 0.0013e^{i3.1} & 0.0001e^{i1.5} & 0.0118
 \end{array}\!\!\right) $
 & $ \begin{array}{c}
0.0105 \\ 
0.0129 \\ 
0.0144 \\ 
0.9622
 \end{array} $
 \\ \hline
Gy & $ \left(\!\!\begin{array}{cccc}
0.4877 & 0.0018e^{i1.5} & 0.4745e^{i1.6} & 0.0013e^{-i1.6} \\ 
0.0018e^{-i1.5} & 0.0123 & 0.0007e^{i0.0} & 0.0001e^{-i2.0} \\ 
0.4745e^{-i1.6} & 0.0007e^{-i0.0} & 0.4879 & 0.0013e^{-i0.0} \\ 
0.0013e^{i1.6} & 0.0001e^{i2.0} & 0.0013e^{i0.0} & 0.0121
 \end{array}\!\!\right) $
 & $ \begin{array}{c}
0.0107 \\ 
0.0123 \\ 
0.0148 \\ 
0.9623
 \end{array} $
 \\ \hline
\end{tabular}

\caption{\textbf{Choi matrix representation of the GST estimated gateset}.  This table lists Choi representations of the estimated gates, and their eigenvalues.  Unitary gates have a spectrum $(1,0,0\ldots)$, just like pure quantum states.  Negative eigenvalues are non-physical, and may represent either statistical fluctuations or violations of the CPTP model used by GST.\label{bestGatesetChoiTable}}
\end{center}
\end{table}



\section{Goodness-of-model Analysis\label{secGoodness}}

The previous section presented the estimated gateset, and compared it to the target gateset.  This section is concerned with a mostly orthogonal analysis which seeks to explain how much the estimated gateset can be trusted -- i.e., how well it fits the data.

To understand the goal of this section, consider the simple problem of fitting a line to a set of points.  For any set of points, there is \emph{always} a best-fit line -- but this doesn't mean that the best-fit line is a \emph{good} fit!  The data points may trace out a parabola, a square, or even something more complicated.  It is essential to understand not just what the best-fit line was (and perhaps how close it was to some desired line), but also \textbf{how well that linear model was able to fit all the data}.  Of course, we do not expect it to fit every data point perfectly.  The critical question is ``Did the linear model fit \emph{as well as we would expect it to} if the data really were generated by a linear process?''

In this analogy, GST's estimated gateset is like the best-fit line, and the target gateset like the desired line.  This section asks the question ``How well was GST able to fit all of the data -- and did it fit well enough to suggest that its model is valid?'' A central tool used to do this is the \emph{likelihood function}, which we denote $\mathcal{L}$, which formally is the probability of the observed data given a set of model parameters.  The basic idea is that we maximize the likelihood function to obtain the best set of model parameters (i.e.~gate set), and by looking at the value of this maximum we can determine the model's goodness-of-fit.  We will actually deal primarily with the logarithm of the likelihood function, $\log(\mathcal{L})$, which is simliarly maximized.

\subsection{Aggregated $\log(\mathcal{L})$}

The log-likelihood for an $n$-outcome system with predicted probabilities $p_i$ and observed frequencies $f_i$ ($i=1\ldots n$) is given by:
\begin{equation}
\log(\mathcal{L}) = \sum_i N f_i \log(p_i).
\end{equation}
where $N$ is the total number of counts. In \emph{this} analysis, $\log(\mathcal{L})$ is used to compare the set of probabilities predicted by a gateset ($p_s$) and the frequencies obtained from a dataset ($f_s$).  Each experiment (or gate sequence) $s$ is associated to two probabilities:  ``plus'' has probability $p_s$ and ``minus'' has probability $1-p_s$.  The $\log(\mathcal{L})$ contribution of a single gate string $s$ is
\begin{equation}
\log(\mathcal{L})_s = N f_s \log(p_s) + N (1-f_s) \log(1-p_s),\label{eqGateStringLogL}
\end{equation}
where $N$ is the number of times the experiment $s$ was performed, $p_s$ is the probability of a ``plus'' outcome as predicted by the gateset, and $f_s$ is the observed frequency of ``plus''.  The total log-likelihood for an entire dataset is just the sum 
\begin{equation}
\log(\mathcal{L}) = \sum_{s\in\mathcal{S}}{ \log(\mathcal{L})_s}.\label{eqDatasetLogL}
\end{equation}
A theoretical upper bound on the log-likelihood can be found by replacing $p_s$ with $f_s$ in Eq.~\ref{eqGateStringLogL} and evaluating Eq.~\ref{eqDatasetLogL}.  We will refer to this quantity as $\log(\mathcal{L})_{ub}$.

Statistical theory has quite a lot to say about the likelihood function (see any of the major textbooks).  Using some of these results, we can predict that if there are $N_p$ free parameters in the gateset that GST is fitting, and GST fits a dataset containing $N_s > N_p$ distinct experiments (gate sequences), then \emph{if the gateset model is correct}, then two times the difference between $\log(\mathcal{L})_{ub}$ and the maximum $\log(\mathcal{L})$ obtained is a random variable with a $\chi^2_{k}$ distribution, where 
$$k \equiv N_s - N_p.$$
Its expected value is $\expec{\chi^2}=k$, and its RMS variance is $\pm\sqrt{2k}$.  Thus, if the fit is ``good'', then twice $\Delta\log(\mathcal{L}) \equiv \log(\mathcal{L})_{ub} - \max(\log(\mathcal{L}))$ should lie roughly within the interval $[k-\sqrt{2k},k+\sqrt{2k}]$ where $k = N_s-N_p$. 
Thus, by comparing the difference $2\Delta\log(\mathcal{L}) - k$ to $\sqrt{2k}$ one can determine how well the GST estimate was able to fit the data in dataset ``$\mathcal{D}$''.

\iftoggle{LsAndGermsSet}{
The MLEGST algorithm used to generate this estimate is iterative.  It starts by fitting only data from the shortest gate sequences (which are easy to fit \emph{and} insensitive to most non-Markovian noise), then successively adds longer and longer sequences (with base sequence length $L\leq 1,2,4,8,\ldots$) to the mix.  Since we get an estimate at each intermediate $L$, it is possible to quantify not just the goodness of the \emph{best} fit (presented in the previous section), but how the goodness-of-fit behaves as longer and longer sequences are added in.

This data is presented in Table \ref{logLProgressTable}.  What you should be looking for here is whether -- at each value of $L$ -- the $2\Delta\log(\mathcal{L})$ quantity is roughly the same as $k$.  More precisely, is $|2\Delta\log(\mathcal{L})-k|$ less than or equal to $\sqrt{2k}$?  If not, then the model is not fitting as well as it should, which usually indicates non-Markovian noise (or, rarely, that the GST algorithm has simply failed to find a good fit even though one exists).  

As a rough rule of thumb, for GST experiments involving relatively long sequences (e.g. $L\geq100$):
%\begin{tabular}{rp{5.5in}}
%$\bigstar\bigstar\bigstar\bigstar\bigstar$ & ``Incredibly good'' experiments have $\chi^2 \approx k$, as predicted by theory (and seen in simulations). \\
%$\bigstar\bigstar\bigstar\bigstar$ & ``Great'' experiments have $\chi^2 \leq 2k$ or so. \\
%$\bigstar\bigstar\bigstar$ & ``Good'' experiments have $\chi^2 \leq 5k$ or so. \\
%$\bigstar\bigstar$ & ``Okay'' experiments have $\chi^2 \leq 10k$. \\
%$\bigstar$ & Experiments in which $\chi^2 > 10k$ have very significant non-Markovian noise, and the results in the previous section should be viewed very cautiously.
%\end{tabular}
\begin{itemize}
\item ``Incredibly good'' ($\bigstar\bigstar\bigstar\bigstar\bigstar$) experiments have $2\Delta\log(\mathcal{L}) \approx k$, as predicted by theory (and seen in simulations).
\item ``Great'' ($\bigstar\bigstar\bigstar\bigstar$) experiments have $2\Delta\log(\mathcal{L}) \leq 2k$ or so.
\item ``Good'' ($\bigstar\bigstar\bigstar$) experiments have $2\Delta\log(\mathcal{L}) \leq 5k$ or so.
\item ``Okay'' ($\bigstar\bigstar$) experiments have $2\Delta\log(\mathcal{L}) \leq 10k$.
\item Experiments in which $2\Delta\log(\mathcal{L}) > 10k$ ($\bigstar$) have very significant non-Markovian noise, and the results in the previous section should be viewed very cautiously.
\end{itemize}

\begin{table}[h]
\begin{center}
\begin{tabular}[l]{|c|c|c|c|c|c|c|c|c|}
\hline
L & $2\Delta\log(\mathcal{L})$ & $k$ & $2\Delta\log(\mathcal{L})-k$ & $\sqrt{2k}$ & $P$ & $N_s$ & $N_p$ & Rating \\ \hline
0 & 53.74648 & 61 & -7.253515 & 11.04536 & 0.73 & 92 & 31 & $\bigstar\bigstar\bigstar\bigstar\bigstar$ \\ \hline
1 & 53.7465 & 61 & -7.253495 & 11.04536 & 0.73 & 92 & 31 & $\bigstar\bigstar\bigstar\bigstar\bigstar$ \\ \hline
2 & 106.745 & 137 & -30.25503 & 16.55295 & 0.97 & 168 & 31 & $\bigstar\bigstar\bigstar\bigstar\bigstar$ \\ \hline
4 & 354.3548 & 410 & -55.64517 & 28.63564 & 0.98 & 441 & 31 & $\bigstar\bigstar\bigstar\bigstar\bigstar$ \\ \hline
8 & 691.0838 & 786 & -94.91617 & 39.64846 & 0.99 & 817 & 31 & $\bigstar\bigstar\bigstar\bigstar\bigstar$ \\ \hline
\end{tabular}

\caption{\textbf{Comparison between the computed and expected maximum $\log(\mathcal{L})$ for different values of $L$}.  $N_S$ and $N_p$ are the number of gate strings and parameters, respectively.  The quantity $2\Delta\log(\mathcal{L})$ measures the goodness of fit of the GST model (small is better) and is expected to lie within $[k-\sqrt{2k},k+\sqrt{2k}]$ where $k = N_s-N_p$. $P$ is the p-value derived from a $\chi^2_k$ distribution (i.e.~when $P <$ some threshold like $0.05$ there is grounds for rejecting the GST fit). The rating from 1 to 5 stars gives a very crude indication of goodness of fit as explained in the text.\label{logLProgressTable}}
\end{center}
\end{table}

}{
\textbf{NOTE: Not enough information was given at the time of this report generation to know provide a goodness-of-model analysis}
}

\FloatBarrier

\iftoggle{LsAndGermsSet}{
\subsection{Detailed likelihood analysis}

The aggregated $2\Delta\log(\mathcal{L})$ numbers presented in Table \ref{logLProgressTable} tell you how well the GST estimate fits the \emph{entire} dataset.  If they are in line with theory ($2\Delta\log(\mathcal{L}) \approx k$), then there is little more to be said.  But if the best fit to the data is not good, we can debug it by identifying \emph{which} experiments are inconsistent with the fit.

Figure \ref{bestGatesetLogLBoxPlot} displays the $2\Delta\log(\mathcal{L})$ contribution from each individual gate sequence (Eq.~\ref{eqGateStringLogL}).  Each gate sequence corresponds to a single colored ``pixel'' in the plot.  Each block of pixels corresponds to a single base sequence (i.e., a germ power), and the individual pixels within a block correspond to the various fiducial sequence pairs between which that base sequence was sandwiched.  (The column indicates the fiducial adjacent to state preparation, while the row indicates the fiducial adjacent to measurement).  Base sequences are arranged in a grid; different rows correspond to different germs, while different columns correspond to different maximum lengths $L$.  Pixels are labeled with the $2\Delta\log(\mathcal{L})$ contribution for that sequence, and colored appropriately.  Shades of blue indicate $2\Delta\log(\mathcal{L})$ contributions in the range 0-3 (``consistent''), while dark red indicates a $2\Delta\log(\mathcal{L})$ contribution of 10 or above.  Such events ($2\Delta\log(\mathcal{L})_s \geq 10$) should occur only once per 638 experiments if the model is correct.

Identifying patterns and trends within such ``pixel plots'' can aid in identifying specific sources and types of non-Markovian noise which may be to blame if the GST algorithms are unable to produce a ``good'' estimate.  For example, it is often the case that all the short sequences [$L = O(1)$] can be fit reasonably well, but the right-hand side of Figure \ref{bestGatesetLogLBoxPlot} becomes a sea of red.  This indicates that non-Markovian behavior (potentially due to slow drift of gateset parameters) is becoming more significant for longer experiments.  In other cases, a single row may be particularly bad, indicating that a particular gate or germ is especially problematic (e.g., was not stabilized using dynamical decoupling techniques).  Be cautious in debugging, however -- sometimes bad $\log(\mathcal{L})$ values for a particular gate or germ can result \emph{not} from faults in that operation, but because another operation failed so badly that it distorted the entire fit (e.g., in trying to fit catastrophically non-Markovian data at Point A, GST ended up failing to fit perfectly good data at Point B).

If the ``generate appendices'' option was selected for this report, similar pixel plots for the intermediate estimates whose total $2\Delta\log(\mathcal{L})$ is listed in Table \ref{logLProgressTable} can be found in Appendix \ref{appendix_logL_pixelplots}.

\begin{figure}
\begin{center}
\includegraphics[width=6.500000in,height=9.000000in,keepaspectratio]{full_reportC_files/bestLogLBoxes.pdf}
\caption{\textbf{$2\Delta\log(\mathcal{L})$ contributions for every individual experiment in the dataset}.  Each pixel represents a single experiment (gate sequence), and its color indicates whether GST was able to fit the corresponding frequency well.  Blue is typical; dark red squares indicating $2\Delta\log(\mathcal{L})_s>10$ should appear only once per 638 experiments on average.  Square blocks of pixels correspond to base sequences (arranged vertically by germ and horizontally by length); each pixel within a block corresponds to a specific choice of pre- and post-fiducial sequences.  See text for further details.\label{bestGatesetLogLBoxPlot}}
\end{center}
\end{figure}

Figure \ref{bestGatesetLogLInvBoxPlot} shows exactly the same $\log(\mathcal{L})$ analysis as Figure \ref{bestGatesetLogLBoxPlot}, but arranged differently.  Here, blocks (not square) all correspond to a single fiducial pair (e.g., pre- and post-fiducial), and pixels within a block correspond to different base sequences.  This can be useful for diagnosing a single bad fiducial sequence.

\begin{figure}
\begin{center}
\includegraphics[width=6.500000in,height=9.000000in,keepaspectratio]{full_reportC_files/bestLogLBoxes_inverted.pdf}
\caption{\textbf{$2\Delta\log(\mathcal{L})$ contributions for each experiment, arranged differently.}  This figure shows the same data as Figure \ref{bestGatesetLogLBoxPlot}, but arranged differently.  Each block now corresponds to a particular pair of fiducial sequences, while pixels within the block correspond to different base sequences sandwiched between those fiducials.\label{bestGatesetLogLInvBoxPlot}}
\end{center}
\end{figure}

}{}



\end{document}